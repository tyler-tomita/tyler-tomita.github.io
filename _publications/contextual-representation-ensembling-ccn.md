---
title: "Contextual Representation Ensembling for Continual Lifelong Learning"
collection: publications
permalink: /publication/contextual-representation-ensembling
excerpt: 'Real-world agents must be able to efficiently acquire new skills over a lifetime, a process called “continual learning.” Current continual machine learning models fall short because they do not selectively and flexibly transfer prior knowledge representations to novel contexts. We propose a cognitively-inspired model called Contextual Representation Ensembling (CRE), which fills this gap. We compared CRE to other state-of-the-art continual machine learning models as well as other baseline models on a simulated continual learning experiment. CRE demonstrated superior transfer to novel contexts and superior remembering when old contexts are re-encountered. Our results suggest that, in order to achieve efficient continual learning in the real world, an agent must have two abilities: (i) they must be able to recognize context cues within the environment in order to infer what prior knowledge might be relevant to the current context and (ii) they must be able to flexibly recombine prior knowledge.'
date: 2022-06
venue: 'Cognitive Computational Neuroscience 2022'
paperurl: 'https://2022.ccneuro.org/proceedings/0000134.pdf'
citation: 'Tomita, Tyler. (2022). &quot;Contextual Representation Ensembling.&quot; <i>Conference on Cognitive Computational Neuroscience 2022</i>. 1(1).'
---
Real-world agents must be able to efficiently acquire new skills over a lifetime, a process called “continual learning.” Current continual machine learning models fall short because they do not selectively and flexibly transfer prior knowledge representations to novel contexts. We propose a cognitively-inspired model called Contextual Representation Ensembling (CRE), which fills this gap. We compared CRE to other state-of-the-art continual machine learning models as well as other baseline models on a simulated continual learning experiment. CRE demonstrated superior transfer to novel contexts and superior remembering when old contexts are re-encountered. Our results suggest that, in order to achieve efficient continual learning in the real world, an agent must have two abilities: (i) they must be able to recognize context cues within the environment in order to infer what prior knowledge might be relevant to the current context and (ii) they must be able to flexibly recombine prior knowledge.

[Download paper here](https://2022.ccneuro.org/proceedings/0000134.pdf)

<!-- Recommended citation: Your Name, You. (2009). "Paper Title Number 1." <i>Journal 1</i>. 1(1). -->
